name: Deploy Vertex AI Pipeline

on:
  workflow_dispatch:
    inputs:
      pipeline:
        description: "Which pipeline to deploy (training|predict|drift-data|drift-concept|drift-full|retrain)"
        required: true
        default: "training"

jobs:
  deploy:
    runs-on: ubuntu-latest

    permissions:
      id-token: write
      contents: read

    env:
      GCP_PROJECT: ${{ secrets.GCP_PROJECT }}
      GCP_REGION:  ${{ secrets.GCP_REGION }}
      ARTIFACT_BUCKET: ${{ secrets.ARTIFACT_BUCKET }}   # gs://your-staging
      SERVICE_ACCOUNT: ${{ secrets.SERVICE_ACCOUNT }}   # sa@project.iam.gserviceaccount.com
      KMS_KEY: ${{ secrets.KMS_KEY }}                   # optional: projects/.../cryptoKeys/...
      PIPELINE_NAME: ${{ github.event.inputs.pipeline }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Auth to Google Cloud (Workload Identity)
        uses: google-github-actions/auth@v2
        with:
          workload_identity_provider: ${{ secrets.WIF_PROVIDER }}
          service_account: ${{ secrets.DEPLOYER_SA }}

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install kfp "google-cloud-aiplatform[pipelines]" google-cloud-storage google-cloud-bigquery

      - name: Compile pipeline(s)
        run: |
          python scripts/compile_and_package.py --only "${PIPELINE_NAME}" --out-dir artifacts
          echo "LATEST_SPEC=$(ls -t artifacts/${PIPELINE_NAME}-*.json | head -n 1)" >> $GITHUB_ENV

      - name: Submit job
        run: |
          echo "Spec: $LATEST_SPEC"
          # only for demonstrating
          python scripts/submit_pipeline_job.py \
            --project "${GCP_PROJECT}" \
            --region "${GCP_REGION}" \
            --staging-bucket "${ARTIFACT_BUCKET}" \
            --service-account "${SERVICE_ACCOUNT}" \
            --pipeline-spec "${LATEST_SPEC}" \
            --job-display-name "${PIPELINE_NAME}-job" \
            --encryption-key "${KMS_KEY}" \
            --param bq_project=${{ secrets.PARAM_BQ_PROJECT }} \
            --param fetch_raw_data_query="SELECT * FROM \`your_project.dataset.train_data\` WHERE DATE <= CURRENT_DATE()" \
            --param date_col=date \
            --param gap=3 \
            --param prediction_window=1 \
            --param k=40000 \
            --param bq_table=dataset.model_eval_result \
            --param selection_metric=recall \
            --param gcs_project=${{ secrets.PARAM_GCS_PROJECT }} \
            --param export_bucket=${{ secrets.PARAM_EXPORT_BUCKET }}
